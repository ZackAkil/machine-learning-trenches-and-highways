{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Trenches.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fmQ3WHxeol9_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZackAkil/machine-learning-trenches-and-highways/blob/master/short/Machine_Learning_Trenches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDqWvp6i9pq",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Trenches\n",
        "\n",
        "This notebook will walk you through the process of building a Machine Learning model for predicting **if a patient has pneumonia or not based on their X-ray image**.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/ZackAkil/machine-learning-trenches-and-highways/master/misc/xray.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcDTotYUjYWu",
        "colab_type": "text"
      },
      "source": [
        "First we will import our data\n",
        "\n",
        "---\n",
        "**Practicle note ‚ö°**: becuase we are importing data direct from Cloud Storage into Colab, the data never touches our local computer and is leveraging the super fast  download speeds between Google services.\n",
        "\n",
        "Download the data using the following command:\n",
        "\n",
        "```bash\n",
        "!gsutil cp \"gs://raw-data-sets/x-ray-pneumonia/training x-ray data.zip\" .\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExYs1tTjXLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [TASK] download data using gsutil \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJGu2nPqmKK",
        "colab_type": "text"
      },
      "source": [
        "Now that the zip file of our data is downloaded, we can unzip it using:\n",
        "```bash\n",
        "\n",
        "!unzip -q \"training x-ray data.zip\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D67XpYaqiLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [TASK] unzip the data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEP9ln-LsEFl",
        "colab_type": "text"
      },
      "source": [
        "## Go OOM or go homeüéà\n",
        "(OOM means \"Out of memory\")\n",
        "\n",
        "Machine learning models love having lots of data to learn from, unfortunatly (and especially for image based models) all that data can quickly fill up the memory of our computers causing OOM (Out of Memory) errors if we are lucky, and more obscure errors when we are not so lucky. \n",
        "\n",
        "There are a couple of ways to work through this problem:\n",
        "\n",
        "-\n",
        "\n",
        "**A.**  Brute force it \n",
        "\n",
        "---\n",
        "You can run the code on a local machine that has more memory, **or** set up and configure your own remote VM with more memory, **or** use a cloud hosted managed notebook service like [AI Platform Noetbooks](https://cloud.google.com/ai-platform-notebooks/) that allows you to easily customise the harware of a remote notebook environment.\n",
        "\n",
        "-\n",
        "\n",
        "**B.** Just use less data\n",
        "\n",
        "---\n",
        "Usually the quickest solution is to just use a sample of your data that fits in memory, however this isn't ideal if you want the best possible model, it is good if you're just experimenting around with POCs (proof of concepts).\n",
        "\n",
        "-\n",
        "\n",
        "**C.** Compress your data (mostly for images)\n",
        "\n",
        "---\n",
        "You don't normally need high resolution images of cats to detect if there is a cat, so it's common to see machine learning models trained on what we would see as \"low resolution\" images e.g 128x128 pixels or even in the case of the famous [MNIST dataset ](https://en.wikipedia.org/wiki/MNIST_database): 28x28 pixels.\n",
        "\n",
        "\n",
        "Example 4mb image of cat\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/random-assets/images/cat_bed_4mb.jpg\" height=200></img>\n",
        "\n",
        "Example 70kb image of cat\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/random-assets/images/cat_bed_80kb.png\" height=200></img>\n",
        "\n",
        "In both images you can clearly see that there is a cat, therefore a machine learning model could learn that there is a cat from both, except for the same memory used in the first (4mb) image you could load in more than 50 (70kb) images  and expect to get the same performance from your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17JJnAVskVFi",
        "colab_type": "text"
      },
      "source": [
        "### Now lets look at some of our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1SGDCMkUjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that lets us navigate files\n",
        "import os\n",
        "\n",
        "# get list of file names in the NORMAL folder (not pneumonia)\n",
        "folder_name = \"NORMAL/\"\n",
        "file_name_list = os.listdir(folder_name)\n",
        "\n",
        "# print out the file name of the first NORMAL image\n",
        "print(folder_name + file_name_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scRPwggAyeD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that lets us load in an image using a file name\n",
        "import skimage.io as io\n",
        "\n",
        "# load image\n",
        "image = io.imread(folder_name + file_name_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FwPlE431OE0",
        "colab_type": "text"
      },
      "source": [
        "Let's just print out the image using the `print()` function, what do you expect to see?\n",
        "```python\n",
        "print(your image varible)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DVhL9nnzLm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [TASK] print out the image you just loaded in\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjxtG-6L0Ehx",
        "colab_type": "text"
      },
      "source": [
        "Numbers??? We'll display the image properly soon enough.\n",
        "\n",
        "You can see the dimensions (resolution) of the image by looking at the `.shape` property of the image like so:\n",
        "\n",
        "```python\n",
        "print(my_image.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk3lJz4mzNRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [TASK] check the dimensions of your image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCoNu7N70lFA",
        "colab_type": "text"
      },
      "source": [
        "Let's actually see the image, there is a very popular data visualisation package called [`matplotlib`](https://matplotlib.org/) that we can use for that. Use the following code to display the image:\n",
        "```python\n",
        "plt.imshow(my_image)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHbla534zOuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that lets us plot charts and graphics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# [TASK] display the image\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS4sYxyiwOci",
        "colab_type": "text"
      },
      "source": [
        "Does the colour of the image look abit strange to you? You can set the **[cmap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)** (colour map) of the image to grayscale if you want the more \"x-ray\" look by changing the code to :\n",
        "\n",
        "```python\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-7V7-GuKLRx",
        "colab_type": "text"
      },
      "source": [
        "## Let's import a lot of data now\n",
        "When dealing with real data it's unlikly to ever be in exactly the right format. This lesson is no different! \n",
        "\n",
        "Our data is stored across multiple folders in diferent types of images (**grayscale** and **<font color='red'>R</font><font color='green'>G</font><font color='blue'>B</font>**) and even different resolutions. A Machine Leanring model **can not** handle different types of images without preprocessing.\n",
        "\n",
        "To save everyone alot of headache below is a function that will do the preprocessing on this data for us. Specifically it will\n",
        "\n",
        "1.   Import each image as a matrix\n",
        "2.   Convert RGB images to grayscale\n",
        "3.   Resize all images to be the same resolution\n",
        "\n",
        "[**TASK**] Read through and run the code to create the function `load_images_from_folder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wii5GnRHodc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that lets you resize images\n",
        "import cv2\n",
        "\n",
        "# import package that will let us combine all the images into one big matrix\n",
        "import numpy as np\n",
        "\n",
        "# create function for preprocessing the images\n",
        "def load_images_from_folder(path, max_to_import=1000, image_resize_shape=(200, 200)):\n",
        "  \n",
        "  print('Started import from {}...'.format(path))\n",
        "  \n",
        "  image_list = []    \n",
        "  image_count = 0\n",
        "  \n",
        "  for img_file_name in os.listdir(path):\n",
        "    \n",
        "      # get all file names that end in 'jpeg', e.i are images       \n",
        "      if img_file_name[-4:] != 'jpeg':\n",
        "        continue\n",
        "      \n",
        "      # import image into matrix format\n",
        "      image = io.imread(os.path.join(path, img_file_name))\n",
        "\n",
        "      image_count +=1\n",
        "      \n",
        "      # annoyingly some of the images are RGB and and have a shape like [480, 640, 3]    \n",
        "      # whilst most others are grayscale and have a shape like [480, 640]. So let's convert\n",
        "      # the multi-channel RGB images to single channel images.       \n",
        "      if len(image.shape) > 2:\n",
        "        image = image[:,:,0]\n",
        "\n",
        "      # resize image and add it to the list of images\n",
        "      image_list.append(cv2.resize(image, dsize=image_resize_shape)) \n",
        "\n",
        "      # every 200 images print out a message so that we know it's doing something      \n",
        "      if (image_count%200)==0:\n",
        "        print('imported ', image_count)\n",
        "\n",
        "      # quit function when reached max_to_import          \n",
        "      if image_count >= max_to_import:\n",
        "        break\n",
        "  \n",
        "  print('...done, showing example image') \n",
        "  \n",
        "  plt.imshow(image_list[0], CMAP='gray')\n",
        "  plt.show()\n",
        "  \n",
        "  return np.stack(image_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UGVRBZ9UrZK",
        "colab_type": "text"
      },
      "source": [
        "Let's use the created function \n",
        "\n",
        "**BUT** you need to make a choice of how much data to import  and what size to set all of the images to:\n",
        "\n",
        "**FYI**: there is 1500 NORMAL images and  4000 PNEUMONIA images, and you could use image shapes of (1, 1) all the way to (2000, 2000). \n",
        "\n",
        "**You're likely going to be experimenting with these values later on.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0gX-uJpCYvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this code to test out what resolution you want to import the images at:\n",
        "\n",
        "# play with the following numbers:\n",
        "TEST_RESIZE_SHAPE = (80, 80)\n",
        "\n",
        "# don't have to change this code\n",
        "test_image = load_images_from_folder('NORMAL/', \n",
        "                                     max_to_import=1, \n",
        "                                     image_resize_shape=TEST_RESIZE_SHAPE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mVwkhx_OHfL",
        "colab_type": "text"
      },
      "source": [
        "*Scroll to the very bottom to see some examples of numbers to use for the next section*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PVEy9s6QT76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [TASK] Pick a number of images to import for each category\n",
        "# and the IMAGE_RESIZE_SHAPE \n",
        "\n",
        "NUMBER_OF_NORMAL_IMAGES_TO_IMPORT = 100 # up to 1500 \n",
        "NUMBER_OF_PNEUMONIA_IMAGES_TO_IMPORT = 100 # up to 4000\n",
        "IMAGE_RESIZE_SHAPE = (80, 80)\n",
        "\n",
        "\n",
        "# don't need to change the following code\n",
        "normal = load_images_from_folder('NORMAL/', \n",
        "                                 max_to_import=NUMBER_OF_NORMAL_IMAGES_TO_IMPORT, \n",
        "                                 image_resize_shape = IMAGE_RESIZE_SHAPE)\n",
        "pneumonia = load_images_from_folder('PNEUMONIA/', \n",
        "                                    max_to_import = NUMBER_OF_PNEUMONIA_IMAGES_TO_IMPORT, \n",
        "                                    image_resize_shape = IMAGE_RESIZE_SHAPE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlXBKTsaVpO5",
        "colab_type": "text"
      },
      "source": [
        "## Building the training dataset üè´\n",
        "\n",
        "We now need to organise our data so that we have the \"**input**\" data to our model (the image of an X-ray) along side the correct answers (the \"**output**\" data). In our case we want our model to output \"**0**\" went it's a \"**normal**\" X-ray, and \"**1**\" when it's and X-ray of \"**pneumonia**\". \n",
        "\n",
        "It's machine learning convention to assign the input data to a varible **`X`** and the output data to a varible **`y`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH7VnBO2VXW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that will convert our outputs into the correct format\n",
        "import keras\n",
        "\n",
        "# join together the normal and pneumonia data \n",
        "X = np.concatenate([normal, pneumonia])\n",
        "y = np.concatenate([np.zeros(len(normal)), np.ones(len(pneumonia))])\n",
        "\n",
        "# bring the input data together and divide all the values by \n",
        "# 255 to get the values between 0 and 1\n",
        "X = X.reshape(len(X), IMAGE_RESIZE_SHAPE[0], IMAGE_RESIZE_SHAPE[1], 1)/255.\n",
        "\n",
        "# convert the labels into the correct format for the neural network\n",
        "y = keras.utils.to_categorical(y, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5yZwlLqBWrw",
        "colab_type": "text"
      },
      "source": [
        "## Train/Test split our data\n",
        "There is no point testing a student with the exact same questions you asked them in a class exersices, they could have just memorised the answers. Machine Learning models are no different. That is why it's important to split our data into a **training** set that it tries to learn from, and a **test** set that we use to check the models accuracy with.\n",
        "\n",
        "  [Sci-Kit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is popular open-source machine learning library with a lot of tools for building models and data proprosessing, including a handy function that will train/test **and** shuffle our data for us. It's important to also shuffle our data as you want an even distribution of 'normal' and 'pneumonia' cases in both the training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNgF87-ivC7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that will do the train/test split and shuffle our data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkBbHwxHXJmz",
        "colab_type": "text"
      },
      "source": [
        "## Time to build a model ü§ñ\n",
        "We are going to use a machine learning package called [Keras](https://keras.io/) that is a populare open-source package used for building neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h4OnD_iXPyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages for building a neural network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvVVoMJP_XWK",
        "colab_type": "text"
      },
      "source": [
        "## Building our network\n",
        "The only thing that is defininate is the input shape layer (it has to match the input data we are using) and the output shape layer (it has to match the output data we are using), all of the inbetween layers are up for experimenting with. \n",
        "\n",
        "**you can add and remove the middle layers if you want to see how it changes the performance and training time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWybtqfWXx-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create our neural network layer by layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(IMAGE_RESIZE_SHAPE[0], IMAGE_RESIZE_SHAPE[1], 1)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05xicYXhX9Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile model to set the optimisation algorithm and metrics we want to see during training\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeSWObNaYRyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model for 5 epochs with our training data, and test using our test data \n",
        "\n",
        "training_job = model.fit(X_train, y_train, \n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVn5xTIzyX_2",
        "colab_type": "text"
      },
      "source": [
        "## Train/Test score curves\n",
        "Let's look at how our models performance changed whilst training. Using the following graphs we can see if our model is learning anything, and if so, if it is **overfitting** to our training data by looking at how close the train and test scores are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAKKylO4ye68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract train (acc) and test (val_acc) scores from our training job\n",
        "train_score = training_job.history['acc']\n",
        "test_score = training_job.history['val_acc']\n",
        "\n",
        "# plot scores\n",
        "plt.plot(train_score, c='b', label='Train Score')\n",
        "plt.plot(test_score, c='r', label='Test Score')\n",
        "plt.ylim(0,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwtuW0Jnz9_k",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a confusion matrix describing the models performance on the test data:\n",
        "\n",
        "You can play with the [confusion matrix playground](https://zackakil.github.io/precision-recall-playground/) to get a feel for what the numbers mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0nmrDKs0Ba3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import package that will generate the confusion matrix scores\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# import packages that will help display the scores\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# generate confusion matrix scores\n",
        "confusion_matrix_scores = confusion_matrix(y_test[:,1], \n",
        "                                           model.predict_classes(X_test))\n",
        "\n",
        "# display scores as a heatmap\n",
        "df = pd.DataFrame(confusion_matrix_scores, \n",
        "                  columns = [\"Predicted Normal\", \"Predicted Pneumonia\"],\n",
        "                  index = [\"Actually Normal\", \"Actually Pneumonia\"])\n",
        "\n",
        "sns.heatmap(df, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHjoFCJHAL6u",
        "colab_type": "text"
      },
      "source": [
        "## If this is your first time with the first neural network structure:\n",
        "It probebly doesn't look very good. Try changing:\n",
        "\n",
        "\n",
        "*   The amount of data and the resize shape of the data \n",
        "*   The second network structure (make sure you change the data stucture to fit into the second network, back in the \"**Some weird data structuring** part of the notebook)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmQ3WHxeol9_",
        "colab_type": "text"
      },
      "source": [
        "## Let's go faster üöÄ\n",
        "\n",
        "Now that we are training a model, we might want it to train faster (your time is the most valuble resource). So let's swtch our run time to one with a GPU.   \n",
        "\n",
        "In Colab the tool bar navigate to: **`Runtime > Change runtime type`**, then select **`GPU`** from the **`Hardware Accelerator`** dropdown.\n",
        "\n",
        "You'll have to re-run all of your code now becuase the environment has to restart to attach the GPU.\n",
        "\n",
        "\n",
        "## Now Re-run the notebook now that you have a GPU enabled environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvLbJZG80d6h",
        "colab_type": "text"
      },
      "source": [
        "# Lets use our model to make predictions ‚öïÔ∏è\n",
        "We can call the `.predict` function on our model to send it predictions. Let's look what the raw prediction output looks like from the model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_C8GcEp0KJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a prediction on the first 5 test images\n",
        "predictions = model.predict(X_test[:5])\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vJsozKS3Fcd",
        "colab_type": "text"
      },
      "source": [
        "Let's display the predictions along side the the true values and the input data (the X-ray image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzCNS28A0t8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for pred, truth, img in zip(predictions, y_test[:5], X_test[:5]):\n",
        "  \n",
        "  # predict pneumonia when the pneumonia class is over 0.5    \n",
        "  if pred[1] > 0.5:\n",
        "    print('predicted: pneumonia')\n",
        "  else:\n",
        "    print('predicted: normal')\n",
        "  \n",
        "  # show the true value of the image   \n",
        "  if truth[1]:\n",
        "    print('truth: pneumonia')\n",
        "  else:\n",
        "    print('truth: normal')\n",
        "  \n",
        "  # show the image  \n",
        "  plt.imshow(np.reshape(img, IMAGE_RESIZE_SHAPE))\n",
        "  plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up3DT8GNwhfK",
        "colab_type": "text"
      },
      "source": [
        "# Done üèÅ\n",
        "\n",
        "Congratulations on building a deep neural network that does image classifciation on x-rays!\n",
        "\n",
        "Now that you have built this model you may want to deploy it to the Cloud so that it can be used by apps, systems, customers etc.\n",
        "\n",
        "There is a tutorial [here](https://codelabs.developers.google.com/codelabs/what-if-tool-caip/) that covers the process of deploying a Keras model to the Cloud that you can do if you are interested. \n",
        "\n",
        "If you'd like to learn more about deep learning (i.e what all the layers in a neural network do) then I highly recommend [this book](https://books.google.co.uk/books?id=Yo3CAQAACAAJ&dq=Deep+Learning+With+Python&hl=en&sa=X&ved=0ahUKEwjk94ado8LkAhWFSxUIHQxwDpkQ6AEIKjAA) by Fran√ßois Chollet (the creator of Keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBJ1oFEUOTWC",
        "colab_type": "text"
      },
      "source": [
        "## But wait, could we have just used [AutoML Image Classification](https://cloud.google.com/automl) to do all of what we just did?\n",
        "\n",
        "...\n",
        "\n",
        "yes (and it would produce a better model **and** deploy it for us)"
      ]
    }
  ]
}